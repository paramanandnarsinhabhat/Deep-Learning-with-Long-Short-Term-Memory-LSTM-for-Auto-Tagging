{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "1. Load Data and Import Libraries\n",
    "2. Text Cleaning\n",
    "3. Merge Tags with Questions\n",
    "4. Dataset Prepartion\n",
    "5. Text Representation\n",
    "6. Model Building\n",
    "    1. Define Model Architecture\n",
    "    2. Train the Model\n",
    "7. Model Predictions\n",
    "8. Model Evaluation\n",
    "9. Inference\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "# Load Data and Import Libraries\n",
    "import re \n",
    "\n",
    "#reading files\n",
    "import pandas as pd\n",
    "\n",
    "#handling html data\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "#visualization\n",
    "import matplotlib.pyplot as plt  \n",
    "\n",
    "\n",
    "pd.set_option('display.max_colwidth', 200)\n",
    "\n",
    "\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "# Specify the path to the zip file\n",
    "zip_file_path = 'data/archive (2).zip'\n",
    "\n",
    "# Specify the directory to extract to\n",
    "extract_to_dir = 'data/unzipped_contents'\n",
    "\n",
    "# Create a directory to extract to if it doesn't exist\n",
    "os.makedirs(extract_to_dir, exist_ok=True)\n",
    "\n",
    "# Open the zip file\n",
    "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "    # Extract all the contents into the directory\n",
    "    zip_ref.extractall(extract_to_dir)\n",
    "    \n",
    "    # List the contents of the extracted folder\n",
    "    print(f\"Contents of the zip file '{zip_file_path}':\")\n",
    "    for file_name in zip_ref.namelist():\n",
    "        print(file_name)\n",
    "\n",
    "# Now you can access files inside the unzipped directory\n",
    "# For example, to open a file:\n",
    "# with open(os.path.join(extract_to_dir, 'yourfile.txt'), 'r') as file:\n",
    "#     print(file.read())\n",
    "        \n",
    "# load the stackoverflow questions dataset\n",
    "questions_df = pd.read_csv('data/unzipped_contents/Questions.csv',encoding='latin-1')\n",
    "\n",
    "# load the tags dataset\n",
    "tags_df = pd.read_csv('data/unzipped_contents/Tags.csv')\n",
    "\n",
    "#print first 5 rows\n",
    "\n",
    "print(questions_df.head())\n",
    "        \n",
    "# Text Cleaning\n",
    "#Let's define a function to clean the text data.\n",
    "\n",
    "def cleaner(text):\n",
    "\n",
    "  text = BeautifulSoup(text).get_text()\n",
    "  \n",
    "  # fetch alphabetic characters\n",
    "  text = re.sub(\"[^a-zA-Z]\", \" \", text)\n",
    "\n",
    "  # convert text to lower case\n",
    "  text = text.lower()\n",
    "\n",
    "  # split text into tokens to remove whitespaces\n",
    "  tokens = text.split()\n",
    "\n",
    "  return \" \".join(tokens)\n",
    "\n",
    "# call preprocessing function\n",
    "questions_df['cleaned_text'] = questions_df['Body'].apply(cleaner)\n",
    "\n",
    "print(questions_df['Body'][1])\n",
    "\n",
    "print(questions_df['cleaned_text'][1])\n",
    "\n",
    "# Merge Tags with Questions\n",
    "print(tags_df.head())\n",
    "\n",
    "# count of unique tags\n",
    "len(tags_df['Tag'].unique())\n",
    "\n",
    "print(len(tags_df['Tag'].unique()))\n",
    "\n",
    "tags_df['Tag'].value_counts()\n",
    "\n",
    "print(tags_df['Tag'].value_counts())\n",
    "\n",
    "# remove \"-\" from the tags\n",
    "tags_df['Tag']= tags_df['Tag'].apply(lambda x:re.sub(\"-\",\" \",x))\n",
    "\n",
    "# group tags Id wise\n",
    "tags_df = tags_df.groupby('Id').apply(lambda x:x['Tag'].values).reset_index(name='tags')\n",
    "tags_df.head()\n",
    "\n",
    "print(tags_df.head())\n",
    "\n",
    "# merge tags and questions\n",
    "df = pd.merge(questions_df,tags_df,how='inner',on='Id')\n",
    "\n",
    "df = df[['Id','Body','cleaned_text','tags']]\n",
    "\n",
    "print(df.head())\n",
    "\n",
    "print(df.shape)\n",
    "\n",
    "# Dataset Preparation\n",
    "# check frequency of occurence of each tag\n",
    "freq= {}\n",
    "for i in df['tags']:\n",
    "  for j in i:\n",
    "    if j in freq.keys():\n",
    "      freq[j] = freq[j] + 1\n",
    "    else:\n",
    "      freq[j] = 1\n",
    "\n",
    "\n",
    "#Let's find out the most frequent tags.\n",
    "# sort the dictionary in descending order\n",
    "freq = dict(sorted(freq.items(), key=lambda x:x[1],reverse=True))\n",
    "\n",
    "print(freq.items())\n",
    "\n",
    "# Top 10 most frequent tags\n",
    "common_tags = list(freq.keys())[:10]\n",
    "print(common_tags)\n",
    "\n",
    "x=[]\n",
    "y=[]\n",
    "\n",
    "for i in range(len(df['tags'])):\n",
    "  \n",
    "  temp=[]\n",
    "  for j in df['tags'][i]:\n",
    "    if j in common_tags:\n",
    "      temp.append(j)\n",
    "\n",
    "  if(len(temp)>1):\n",
    "    x.append(df['cleaned_text'][i])\n",
    "    y.append(temp)\n",
    "\n",
    "# number of questions left\n",
    "len(x)\n",
    "\n",
    "print(len(x))\n",
    "\n",
    "y[:10]\n",
    "\n",
    "print(y[:10])\n",
    "#Now we will find a suitable sequence length.\n",
    "#We will the input sequences to our model to the length of 100\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "mlb = MultiLabelBinarizer()\n",
    " \n",
    "y = mlb.fit_transform(y)\n",
    "y.shape\n",
    "\n",
    "\n",
    "print(y[0,:])\n",
    "\n",
    "print(mlb.classes_)\n",
    "\n",
    "#We can now split the dataset into training set and validation set. \n",
    "from sklearn.model_selection import train_test_split\n",
    "x_tr,x_val,y_tr,y_val=train_test_split(x, y, test_size=0.2, random_state=0,shuffle=True)\n",
    "\n",
    "# Text Representation\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences \n",
    "\n",
    "#prepare a tokenizer\n",
    "x_tokenizer = Tokenizer() \n",
    "\n",
    "#prepare vocabulary\n",
    "x_tokenizer.fit_on_texts(x_tr)\n",
    "\n",
    "print(x_tokenizer.word_index)\n",
    "\n",
    "print(len(x_tokenizer.word_index))\n",
    "\n",
    "'''\n",
    "There are around 25,000 tokens in the training dataset. Let's see how many tokens appear at least 5 times in the dataset.\n",
    "'''\n",
    "\n",
    "thresh = 3\n",
    "\n",
    "cnt=0\n",
    "for key,value in x_tokenizer.word_counts.items():\n",
    "  if value>=thresh:\n",
    "    cnt=cnt+1\n",
    "\n",
    "print(cnt)\n",
    "\n",
    "# prepare the tokenizer again\n",
    "x_tokenizer = Tokenizer(num_words=cnt,oov_token='unk')\n",
    "\n",
    "#prepare vocabulary\n",
    "x_tokenizer.fit_on_texts(x_tr)\n",
    "\n",
    "#define threshold for maximum length of a setence\n",
    "max_len=100\n",
    "\n",
    "#convert text sequences into integer sequences\n",
    "x_tr_seq = x_tokenizer.texts_to_sequences(x_tr) \n",
    "x_val_seq = x_tokenizer.texts_to_sequences(x_val)\n",
    "\n",
    "#padding up with zero \n",
    "x_tr_seq = pad_sequences(x_tr_seq,  padding='post', maxlen=max_len)\n",
    "x_val_seq = pad_sequences(x_val_seq, padding='post', maxlen=max_len)\n",
    "\n",
    "#no. of unique words\n",
    "x_voc_size = x_tokenizer.num_words + 1\n",
    "x_voc_size\n",
    "\n",
    "print(x_voc_size)\n",
    "\n",
    "x_tr_seq[0]\n",
    "\n",
    "print(x_tr_seq[0])\n",
    "\n",
    "# Model Building\n",
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "from keras.callbacks import *\n",
    "\n",
    "### Define Model Architecture\n",
    "#sequential model\n",
    "model = Sequential()\n",
    "\n",
    "#embedding layer\n",
    "model.add(Embedding(x_voc_size, 50, trainable = True, input_shape=(max_len,),mask_zero=True))\n",
    "\n",
    "#lstm \n",
    "model.add(LSTM(128))\n",
    "\n",
    "#dense layer\n",
    "model.add(Dense(128,activation='relu')) \n",
    "\n",
    "#output layer\n",
    "model.add(Dense(10,activation='sigmoid'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "#define optimizer and loss\n",
    "model.compile(optimizer='adam',loss='binary_crossentropy')\n",
    "\n",
    "#checkpoint to save best model during training\n",
    "mc = ModelCheckpoint(\"weights.best.hdf5\", monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "### Train the Model\n",
    "#train the model \n",
    "model.fit(x_tr_seq, y_tr, batch_size=128, epochs=10, verbose=1, validation_data=(x_val_seq, y_val), callbacks=[mc])\n",
    "\n",
    "# Model Predictions \n",
    "# load weights into new model\n",
    "model.load_weights(\"weights.best.hdf5\")\n",
    "\n",
    "#predict probabilities\n",
    "pred_prob = model.predict(x_val_seq)\n",
    "\n",
    "print(pred_prob[0])\n",
    "\n",
    "import numpy as np\n",
    "#define candidate threshold values\n",
    "threshold  = np.arange(0,0.5,0.01)\n",
    "print(threshold)\n",
    "\n",
    "# convert probabilities into classes or tags based on a threshold value\n",
    "def classify(pred_prob,thresh):\n",
    "  y_pred_seq = []\n",
    "\n",
    "  for i in pred_prob:\n",
    "    temp=[]\n",
    "    for j in i:\n",
    "      if j>=thresh:\n",
    "        temp.append(1)\n",
    "      else:\n",
    "        temp.append(0)\n",
    "    y_pred_seq.append(temp)\n",
    "\n",
    "  return y_pred_seq\n",
    "\n",
    "from sklearn import metrics\n",
    "score=[]\n",
    "\n",
    "#convert to 1 array\n",
    "y_true = np.array(y_val).ravel() \n",
    "\n",
    "for thresh in threshold:\n",
    "    \n",
    "    #classes for each threshold\n",
    "    y_pred_seq = classify(pred_prob,thresh) \n",
    "\n",
    "    #convert to 1d array\n",
    "    y_pred = np.array(y_pred_seq).ravel()\n",
    "\n",
    "    score.append(metrics.f1_score(y_true,y_pred))\n",
    "\n",
    "# find the optimal threshold\n",
    "opt = threshold[score.index(max(score))]\n",
    "opt\n",
    "\n",
    "print(opt)\n",
    "\n",
    "# Model Evaluation\n",
    "#predictions for optimal threshold\n",
    "y_pred_seq = classify(pred_prob,opt)\n",
    "y_pred = np.array(y_pred_seq).ravel()\n",
    "\n",
    "print(metrics.classification_report(y_true,y_pred))\n",
    "\n",
    "'''\n",
    "## How to improve Model's Performance?\n",
    "\n",
    "1. You can add more LSTM layers\n",
    "2. You can pass pre-trained word embeddings\n",
    "3. You can play with different optimizers\n",
    "4. Try different input sequence length\n",
    "\n",
    "'''\n",
    "y_pred = mlb.inverse_transform(np.array(y_pred_seq))\n",
    "y_true = mlb.inverse_transform(np.array(y_val))\n",
    "\n",
    "df = pd.DataFrame({'comment':x_val,'actual':y_true,'predictions':y_pred})\n",
    "\n",
    "print(df.head())\n",
    "\n",
    "def predict_tag(comment):  \n",
    "  text=[]\n",
    "\n",
    "  #preprocess  \n",
    "  text = [cleaner(comment)]\n",
    "\n",
    "  #convert to integer sequences\n",
    "  seq = x_tokenizer.texts_to_sequences(text)\n",
    "\n",
    "  #pad the sequence\n",
    "  pad_seq = pad_sequences(seq,  padding='post', maxlen=max_len)\n",
    "\n",
    "  #make predictions\n",
    "  pred_prob = model.predict(pad_seq)\n",
    "  classes = classify(pred_prob,opt)[0]\n",
    "  \n",
    "  classes = np.array([classes])\n",
    "  classes = mlb.inverse_transform(classes)  \n",
    "  return classes\n",
    "\n",
    "comment = \"For example, in the case of logistic regression, the learning function is a Sigmoid function that tries to separate the 2 classes\"\n",
    "\n",
    "print(\"Comment:\",comment)\n",
    "print(\"Predicted Tags:\",predict_tag(comment))\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
